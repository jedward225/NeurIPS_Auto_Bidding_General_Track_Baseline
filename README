这是 2025 AI+创研课的集中仓库
Team leader:   刘嘉俊
Team members:  左宇钒 黄宜君 陈庆笠 钟梓洋 叶栩言

每次你输入一个关键词，系统都会弹出一个商品页面。其实，这个页面大有讲究：哪些商品会出现，哪个商品排在前面都是系统精密计算的结果，其中的广告是平台运行拍卖机制把广告位分配给广告主的结果。

整个过程的运行逻辑如下：首先，平台会通过分析用户的兴趣和行为模式来构建用户画像。当用户在淘宝上进行搜索或浏览商品时，平台会立即在后台启动广告拍卖流程。广告主们通过竞价机制参与到这场拍卖中，希望能够让自己的广告获得展示机会。自动出价系统在这个过程中发挥着核心作用，它综合考虑用户的画像、行为数据、广告主的推广目标、预算限制，以及拍卖环境中的多种因素，实时计算出最优化的出价策略。平台会根据这些数据和计算结果，选择出价最高且与用户需求最相关的广告。这些广告结果会和自然结果一起展现给用户。整个过程完全自动化，能在极短的时间内完成。

从这个过程可以看出，**通过自动出价系统，广告主能够大大简化广告投放的流程，利用人工智能技术实现精准营销，从而节省时间和精力。**

**「自动出价」优化之路**

**从强化学习到生成式 AI**

总体来看，整个出价领域业界的方法经历了四代演化。在自动出价策略的优化上，阿里妈妈也进行了多年的研究。

- **第一代：经典控制类。**把效果最大化的优化问题间接转化为预算消耗的控制问题。基于业务数据计算消耗曲线，控制预算尽可能按照设定的曲线来消耗。PID 及相关改进是这一阶段常用的控制算法。当竞价流量价值分布稳定的情况下，这类算法能基本满足业务上线之初的效果优化。
- **第二代：规划求解类。**相比于第一代，规划求解类（LP）算法直接面向目标最大化来进行求解。可基于前一天的参竞流量来预测当前未来流量集合，从而求解出价参数。自动出价问题根据当前已投放的数据变成新的子问题，因此可多次持续地用该方法进行求解，即 Online LP。这类方法依赖对未来参竞流量的精准预估，因此在实际场景落地时需要在未来流量的质和量的预测上做较多的工作。
- **第三代：强化学习类。**现实环境中在线竞价环境是非常复杂且动态变化的，未来的流量集合也是难以精准预测的，要统筹整个预算周期投放才能最大化效果。作为典型的序列决策问题，第三阶段用强化学习类方法来优化自动出价策略。其迭代过程从早期的经典强化学习方法落地，到进一步基于 Offline RL 方法逼近「在线真实环境的数据分布」，再到末期贴近问题本质基于 Online RL 方法实现和真实竞价环境的交互学习。
- **第四代：生成模型类。**以 ChatGPT 为代表的生成式大模型以汹涌澎湃之势到来，在多个领域都表现出令人惊艳的效果。新的技术理念和技术范式可能会给自动出价算法带来革命性的升级。阿里妈妈技术团队提前布局，以智能营销决策大模型 AIGA（AI Generated Action）为核心重塑了广告智能营销的技术体系，并衍生出以 AIGB（AI Generated Bidding）为代表的自动出价策略。



参考的资料：

[与1500多支国内外队伍同台竞技，快手在NeurIPS 2024顶级大赛中上演双杀_腾讯新闻](https://news.qq.com/rain/a/20241216A053AP00)

一文读懂互联网广告拍卖的经典机制设计 - 石士的文章 - 知乎
https://zhuanlan.zhihu.com/p/617582638

Hu, Yudong, Congying Han, Tiande Guo, and Hao Xiao.
“Applying Opponent Modeling for Automatic Bidding in Online Repeated Auctions.” arXiv, February 27, 2024。
https://arxiv.org/pdf/2212.02723

Mou, Zhiyu, Miao Xu, Rongquan Bai, Zhuoran Yang, Chuan Yu, Jian Xu, and Bo Zheng.
“Nash Equilibrium Constrained Auto-Bidding With Bi-Level Reinforcement Learning.” arXiv, March 13, 2025.
https://arxiv.org/pdf/2503.10304


1. regret-Net bidding
2. https://arxiv.org/abs/2409.03052
3. NIPS - baseline (how to bid) and connect with the former Net








Some notes:

1. 数据大小大致有80个G

2. > [广告出价策略](https://www.ichdata.com/tag/广告出价策略)可以分为手动出价和智能自动出价。
   >
   > ## 手动出价
   >
   > ### 基于时间
   >
   > - ~~[CPD](https://www.ichdata.com/tag/cpd)：Cost Per Day，每天成本，广告主按天数支付费用，适用于品牌展示广告，提升品牌知名度。实际上这是广告主的**定价**。~~
   >
   >  
   >
   > ### 基于展示
   >
   > - [CPM](https://www.ichdata.com/tag/cpm)：Cost Per Mille, 每千次展示成本，广告主为每1000次广告展示支付固定的费。适用于品牌展示广告，提升品牌知名度
   >
   >  
   >
   > ### 基于点击
   >
   > - [CPC](https://www.ichdata.com/tag/cpc)：Cost Per Click, 每点击成本，广告主只为点击广告的用户付费。适用于直接响应广告，引导流量到网站或特定页面。
   >
   >  
   >
   > ### 基于行动
   >
   > - [CPA](https://www.ichdata.com/tag/cpa)：Cost Per Action, 每行动成本，广告主为每一次特定的用户行为（如注册、购买等）付费。适用于转化目标明确的广告活动，强调效果和ROI。
   > - CPL：Cost Per Lead, 每潜在客户成本，广告主为每一个潜在客户（比如填写表单的用户）支付费用。适用于需要收集用户信息的营销活动。
   > - [CPS](https://www.ichdata.com/tag/cps)：Cost Per Sale, 每销售成本，广告主为每笔成功的销售付费。适用于电商或产品销售活动。
   > - [CPI](https://www.ichdata.com/tag/cpi)： Cost Per Install，每次安装成本，广告主为每次APP安装支付费用。适用于APP下载。
   > - CPE：Cost Per Engagement，每次互动成本，广告主为特定互动支付费用。适用于电商或产品销售活动。
   >
   >  
   >
   > ### 基于视频
   >
   > - CPV：Cost Per View, 每观看成本，通常用于视频广告，广告主支付每个视频观看的费用。适用于视频营销活动，注重视频内容的传播。
   > - t[CPM](https://www.ichdata.com/tag/cpm)：Target cost-per-thousand impressions，目标每千次展示费用，可以设定你愿意为每一千次展示支付的平均金额。此出价策略会优化出价，以便最大限度地扩大广告系列的唯一身份用户覆盖面。这种主要在Youtube上，
   > - [vCPM](https://www.ichdata.com/tag/vcpm)：Viewable Cost Per Mille, 可视每千次展示成本，类似于[CPM](https://www.ichdata.com/tag/cpm)，但根据广告的可见性（如至少50%的广告曝光时间在屏幕上）来计算。适用于关注广告实际视觉效果的品牌广告。
   >
   >  
   >
   > ## 智能自动出价
   >
   > 智能出价是一种出价策略，它会在每一次竞价时使用算反，以尽可能提高转化次数或转化价值为目标来优化出价，往往需要积累一定的数据（转化）后才可以开启。
   >
   > - o[CPM](https://www.ichdata.com/tag/cpm)：Optimized Cost Per Mille, 优化每千次展示成本，选择特定优化目标，并提供期望平均转化成本后，系统预估每一次展示的转化价值，自动出价，按照展示扣费。适用于希望在展示广告中实现特定性能目标。
   > - [dCPM](https://www.ichdata.com/tag/dcpm)：Dynamic Cost Per Mille, 动态每千次展示成本，主要在DSP中使用，出价根据实时数据动态调整，每千次展示的成本可以根据用户价值变化。适用于结合程序化广告和实时竞价，需要精细化广告投放。
   > - e[CPC](https://www.ichdata.com/tag/cpc)：Enhanced Cost Per Click, 增强每点击成本，属于半智能出价，在[CPC](https://www.ichdata.com/tag/cpc)基础上，系统自动调整出价以优化点击率或转化。适用于需要在预算内最大化点击量或转化的广告主。
   > - o[CPC](https://www.ichdata.com/tag/cpc)：Optimized Cost Per Click，优化版[CPC](https://www.ichdata.com/tag/cpc)，属于智能自动出价，选择特定优化目标，并提供期望平均转化成本后，系统预估每一次展示的转化价值，自动出价，按照点击扣费。
   > - [oCPA](https://www.ichdata.com/tag/ocpa)：Optimized [CPA](https://www.ichdata.com/tag/cpa)，优化版[CPA](https://www.ichdata.com/tag/cpa)，属于智能自动出价，在保障转化成本的前提下，系统依据用户对广告主的价值及竞争环境，自主出价。
   > - ROAS：Return On Ad Spend, 广告支出回报率（不是一种出价策略，但可以用来设定目标），根据设定的广告支出回报率目标来调整出价，Ads将ROAS划分到智能自动出价策略。适用于需要关注广告支出的直接回报的广告主，特别是在电商领域。
   >
   > 手动出价和智能自动出价的关系如下：
   >
   > |                      **售卖方式**                       |                         **出价策略**                         |                                                              |
   > | :-----------------------------------------------------: | :----------------------------------------------------------: | ------------------------------------------------------------ |
   > |                      **手动出价**                       |                       **智能自动出价**                       |                                                              |
   > |                  CPT（Cost Per Time）                   | ~~[CPD](https://www.ichdata.com/tag/cpd)（Cost Per Day）这实际是广告主定价~~ |                                                              |
   > | [PPM](https://www.ichdata.com/tag/ppm)（Pay Per Mille） |   [CPM](https://www.ichdata.com/tag/cpm)（Cost Per Mille）   | o[CPM](https://www.ichdata.com/tag/cpm)（Optimized Cost Per Mille） [dCPM](https://www.ichdata.com/tag/dcpm)（Dynamic Cost Per Mille） |
   > | [PPC](https://www.ichdata.com/tag/ppc)（Pay Per Click） |   [CPC](https://www.ichdata.com/tag/cpc)（Cost Per Click）   | e[CPC](https://www.ichdata.com/tag/cpc)（Enhanced Cost Per Click） o[CPC](https://www.ichdata.com/tag/cpc)（Optimized Cost Per Click） |
   > |                  PPA（Pay Per Action）                  | [CPA](https://www.ichdata.com/tag/cpa)（Cost Per Action） [CPD](https://www.ichdata.com/tag/cpd)(Cost Per Download) [CPI](https://www.ichdata.com/tag/cpi)(Cost Per Install) [CPS](https://www.ichdata.com/tag/cps)(Cost Per Sales) | [oCPA](https://www.ichdata.com/tag/ocpa)（Optimized Cost Per Action） |
   >
   > 延伸阅读：[深入解析：CPC、eCPC与oCPC——广告出价策略的演进](https://www.ichdata.com/cpc-ecpc-ocpc.html)

```
环境和日志设置:
导入必要的库，包括 numpy (用于数值计算)，logging (用于记录训练过程信息)，pandas (用于数据处理)，ast (用于安全地转换字符串到 Python 对象) 以及项目内部定义的 IQL 相关模块 (ReplayBuffer, IQL, utils)。
设置 STATE_DIM = 16，定义了状态空间的维度。
配置了日志记录的格式和级别。
train_iql_model() 函数 - 模型训练的核心流程:
加载数据:
从 ./data/traffic/training_data_rlData_folder/training_data_all-rlData.csv 文件中读取预处理好的强化学习训练数据。这个数据是 rl_data_generator.py 生成的。
数据预处理:
safe_literal_eval 函数：由于 CSV 文件中 state 和 next_state 列可能是以字符串形式存储的列表或元组，该函数使用 ast.literal_eval 将这些字符串安全地转换为 Python 的列表/元组对象。如果转换失败或值为空，则保持原样。
对 "state" 和 "next_state" 列应用 safe_literal_eval。
数据归一化 (Normalization):
is_normalize = True 表明启用了归一化。
normalize_state(training_data, STATE_DIM, normalize_indices=[13, 14, 15]): 对状态数据进行归一化。特别地，指定了状态向量中的第 13, 14, 15 个特征需要归一化（这些索引对应 historical_volume, last_3_timeStepIndexs_volume, timeStepIndex_volume_agg 等特征，这些特征的值域可能较大）。函数会返回一个包含均值和标准差等信息的字典 normalize_dic，用于后续的归一化和反归一化。
training_data['reward'] = normalize_reward(training_data, "reward_continuous"): 对奖励进行归一化。这里选择了 reward_continuous (连续奖励) 进行归一化。脚本中注释掉了使用 reward (稀疏奖励) 的选项。
save_normalize_dict(normalize_dic, "saved_model/IQLtest"): 将归一化过程中产生的字典（包含均值、标准差等）保存到指定路径，以便在模型评估或部署时使用相同的归一化参数。
构建经验回放缓冲区 (Replay Buffer):
replay_buffer = ReplayBuffer(): 初始化一个经验回放缓冲区。强化学习中，经验回放缓冲区用于存储智能体与环境交互产生的经验（(状态, 动作, 奖励, 下一个状态, 是否结束)元组），并从中随机采样进行训练，以打破数据相关性，提高训练稳定性。
add_to_replay_buffer(replay_buffer, training_data, is_normalize): 将处理后的训练数据填充到经验回放缓冲区中。
模型训练:
model = IQL(dim_obs=STATE_DIM): 初始化 IQL 模型。dim_obs 参数指定了观测空间（即状态空间）的维度。
train_model_steps(model, replay_buffer): 调用该函数执行模型的实际训练步骤。
保存模型:
model.save_jit("saved_model/IQLtest"): 将训练好的 IQL 模型保存到指定路径。save_jit 可能表示使用了 TorchScript JIT (Just-In-Time) 编译器进行模型序列化，以便于跨平台部署或在没有 Python 环境的情况下运行。
测试训练好的模型:
test_trained_model(model, replay_buffer): 调用该函数对训练好的模型进行简单的测试。
add_to_replay_buffer() 函数 - 填充经验回放缓冲区:
遍历训练数据中的每一行（每一条经验）。
根据 is_normalize 的值，选择使用原始的状态/奖励还是归一化后的状态/奖励。
一个关键点：if done != 1:，当一条经验的结束标志 done 不为 1 时，正常将 (state, action, reward, next_state, done) 推入缓冲区。
else: 当 done == 1 时，表示这是一个终止状态，此时 next_state 被替换为 np.zeros_like(state)（一个与 state 形状相同的零数组），然后推入缓冲区。这是因为在终止状态之后没有实际的“下一个状态”，通常用零向量或特定标记表示。
train_model_steps() 函数 - 模型训练步骤:
step_num=20000, batch_size=100: 定义了训练的总步数和每个批次的大小。
循环 step_num 次：
states, actions, rewards, next_states, terminals = replay_buffer.sample(batch_size): 从经验回放缓冲区中随机采样一个批次的数据。
q_loss, v_loss, a_loss = model.step(states, actions, rewards, next_states, terminals): 执行 IQL 模型的一个训练步骤。IQL 算法通常会同时学习一个 Q 函数 (state-action value), 一个 V 函数 (state value) 和一个隐式的优势函数 (Advantage function, A = Q - V)。model.step() 内部会计算这些函数的损失并进行梯度下降更新。
logger.info(...): 记录当前训练步数以及 Q 函数损失 (q_loss)、V 函数损失 (v_loss) 和优势函数/策略损失 (a_loss)。
test_trained_model() 函数 - 测试模型:
从回放缓冲区中采样一批数据。
pred_actions = model.take_actions(states): 使用训练好的模型根据当前状态 states 预测动作。
比较模型预测的动作 pred_actions 和数据中真实的动作 actions，并将它们拼接起来打印输出，用于初步评估模型性能。
run_iql() 函数和 if __name__ == '__main__'::
run_iql() 函数简单地调用 train_iql_model() 来启动整个训练流程。
if __name__ == '__main__': 确保当该脚本作为主程序执行时，会调用 run_iql()。
总结一下 IQL 方法在这个脚本中的体现：
离线学习 (Offline Learning): IQL 是一种适用于离线强化学习的算法。这个脚本从一个固定的、预先收集好的数据集 (training_data_all-rlData.csv) 中学习，而不是通过与环境实时交互来学习。
隐式 Q 学习: IQL 的核心思想是通过学习状态价值函数 (V) 和优势加权回归 (Advantage-Weighted Regression, AWR) 来间接优化策略，而不是直接最大化 Q 值。它期望学习到的策略能够匹配数据集中高回报动作的分布。
组件: 训练过程涉及了 IQL 的关键组件：
状态价值函数 (V-function): 评估在某个状态下有多好。
Q 函数 (Q-function): 评估在某个状态下执行某个动作有多好。
优势函数 (Advantage Function): 通常 A(s,a) = Q(s,a) - V(s)，表示在状态 s 下执行动作 a 相对于平均水平的好坏程度。IQL 可能以一种隐式的方式使用它。
损失函数: q_loss, v_loss, a_loss 反映了 IQL 算法中用于更新这些不同函数组件的损失。具体的计算方式在 IQL类的 step 方法中定义（脚本中未提供其内部实现）。
```